import pandas as pd

import numpy as np

import os


from azure.storage.blob import ContainerClient
from fastavro import reader, writer, parse_schema
import io
import json

from forma_LA.constants import (
    INTERVAL_NEW_SESSION,
    GROUPING_VARS,
    GRID_WIDTH,
    EVENTS_COLUMNS,
    BACKUP_INTERMEDIATE_CONTAINER_NAME,
)

MAX_EVENTS = int(os.environ["MAX_EVENTS"])

from forma_LA.schemes import events_metadata_scheme

from forma_LA.container import Container


class Events:
    def __init__(self, container_name, bin_container_name=None, after=""):
        self.__retrieve_events(container_name, bin_container_name, after)

        self.dataframe = pd.DataFrame(self.__events, dtype=str)

        if self.dataframe.shape[0] > 0:
            # In some old events avro, problem: we drop them
            self.dataframe.drop(
                index=self.dataframe.index[self.dataframe["percentage"] == ""], inplace=True
            )  # hay que quitarlo

            self.dataframe.drop(columns=["_id", "state"], inplace=True, errors="ignore")

            self.dataframe = self.dataframe.astype(
                {
                    "percentage": float,
                    "timestamp": float,
                }
            )
            if "time_spent" in self.dataframe.columns:
                self.dataframe = self.dataframe.astype({"time_spent": float})

            self.dataframe["day"] = pd.to_datetime(
                self.dataframe["timestamp"], unit="s"
            ).dt.floor("D")

            self.__add_author_unit()

            ## only because events can be generated by units version < 2.4
            self.__fix_activity_title()

            self.dataframe.sort_values(["timestamp"], inplace=True)

    def __retrieve_events(self, container_name, bin_container_name=None, after=""):
        events_container = Container(container_name).container
        if bin_container_name is not None:
            bin_container = Container(bin_container_name).container

        blob_list = [
            blob for blob in events_container.list_blobs() if blob.name > after
        ]
        blob_list.sort(key=lambda b: b.name)

        if len(blob_list) > 0:
            self.batch_first_events_file = blob_list[0].name

        else:
            self.batch_first_events_file, self.batch_last_events_file = (None, None)

        self.__events = []

        events_number = 0

        for i_blob, blob in enumerate(blob_list):
            if blob.size > 508:

                blob_client = ContainerClient.get_blob_client(
                    events_container, blob=blob.name
                )
                fileReader = blob_client.download_blob().readall()
                print("Downloaded a non empty blob: " + blob.name)

                events_list = self.__process_blob(fileReader)

                events_number += len(events_list)

                if (events_number > MAX_EVENTS) & (i_blob > 1):
                    
                    break

                self.batch_last_events_file = blob.name

                self.__events += events_list

                if bin_container_name is not None:
                    ContainerClient.upload_blob(
                        bin_container, name=blob.name, data=fileReader, overwrite=True
                    )
                    events_container.delete_blob(blob.name)

                    
        print(f"Number of downloaded events: {len(self.__events)}")
        events_container.close()

        if bin_container_name is not None:
            bin_container.close()

    def __process_blob(self, filename):
        with io.BytesIO(filename) as f:
            events_list = []

            avro_reader = reader(f)

            for reading in avro_reader:
                parsed_json = json.loads(reading["Body"])

                events_list.append(parsed_json)

        return events_list

    def upload_metadata(self):
        self.__upload_metadata(
            BACKUP_INTERMEDIATE_CONTAINER_NAME,
            "events_metadata.avro",
            events_metadata_scheme,
        )

    def __upload_metadata(self, container_name, path, schema):
        container = Container(container_name)
        fo = io.BytesIO()
        writer(
            fo,
            parse_schema(schema),
            [
                {
                    "batch_first_events_file": self.batch_first_events_file,
                    "batch_last_events_file": self.batch_last_events_file,
                }
            ],
        )
        ContainerClient.upload_blob(
            container.container, name=path, data=fo.getvalue(), overwrite=True
        )
        container.close()

    def add_unit_type(self):
        if self.dataframe.shape[0] == 0:
            return None

        evaluation_units_urls = ["ed12ad9791554f32b3327671030c0e5e"]  # complete

        if not "unit_type" in self.dataframe:
            self.dataframe["unit_type"] = "Content"
        else:
            self.dataframe["unit_type"] = self.dataframe["unit_type"].where(
                ~self.dataframe["unit_type"].isna(), "Content"
            )

        self.dataframe["unit_type"] = self.dataframe["unit_type"].where(
            ~self.dataframe["url"].isin(evaluation_units_urls), "Evaluation"
        )

    def __add_author_unit(self):
        if self.dataframe.shape[0] > 0:

            urls = self.dataframe["url"]

            url_author = urls.str.contains("/") & (urls != "/la/")

            if url_author.any():
                authors = pd.Series("anonymous", index=urls.index)

                self.dataframe["author"] = authors.where(
                    ~url_author, urls.str.split("/", expand=True).iloc[:, 1]
                )

                self.dataframe["unit"] = urls.where(
                    ~url_author, urls.str.split("/", expand=True).iloc[:, 2]
                )
            else:
                self.dataframe["author"] = "anonymous"
                self.dataframe["unit"] = self.dataframe["url"]

    def __fix_activity_title(self):
        if not ("activity_title" in self.dataframe):
            self.dataframe["activity_title"] = self.dataframe["title"]
        else:
            to_change = self.dataframe["activity_title"] == "nan"
            self.dataframe.loc[to_change, "activity_title"] = self.dataframe.loc[
                to_change, "title"
            ]
